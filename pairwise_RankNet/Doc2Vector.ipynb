{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1w9o6L6sj_N"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import pandas as pd\n",
        "import os\n",
        "from six.moves import urllib\n",
        "import zipfile\n",
        "import tarfile\n",
        "import urllib\n",
        "import pandas as pd \n",
        "import gzip\n",
        "#Import all the dependencies\n",
        "import gensim\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk.tokenize import word_tokenize\n",
        "import os\n",
        "from google.colab import drive\n",
        "import copy\n",
        "import numpy as np\n",
        "import copy\n",
        "from gensim.models.doc2vec import Doc2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the file"
      ],
      "metadata": {
        "id": "Cn3NhV9j7L9J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sanlAj4peWaQ"
      },
      "outputs": [],
      "source": [
        "collection =  pd.read_csv('collection.tsv', sep='\\t', header=None, names=[ 'pid', 'passage' ])\n",
        "collection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = collection\n",
        "doc['passage'] = doc['passage'].str.lower()\n",
        "doc['passage'] = doc['passage'].str.replace('[^a-zA-Z\\s+]', '')\n",
        "doc['passage'] = doc['passage'].str.replace('[\\s+]', ' ')"
      ],
      "metadata": {
        "id": "0fvwoPFsJdWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove Stop Words"
      ],
      "metadata": {
        "id": "e7gZ5iNM7SFj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTShUziQObYK"
      },
      "outputs": [],
      "source": [
        "# Removing Stop Words\n",
        "# 33532\n",
        "# from nltk.corpus import stopwords\n",
        "# nltk.download('stopwords')\n",
        "# for i in range(len(doc['passage'] )):\n",
        "#     doc['passage'][i] = [w for w in doc['passage'][i] if w not in stopwords.words('english')]\n",
        "# # queries_eval['query'] = queries_eval.apply(lambda row: for w in row['query'] if w not in stopwords.words('english'), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODeB_yI2Z4aG"
      },
      "source": [
        "Training "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import smart_open\n",
        "\n",
        "def read_corpus(fname, tokens_only=False):\n",
        "    # with smart_open.open(fname, encoding=\"utf-8\") as f:\n",
        "    for index, row in fname.iterrows():\n",
        "        tokens = gensim.utils.simple_preprocess(row['passage'])\n",
        "        if tokens_only:\n",
        "            yield tokens\n",
        "        else:\n",
        "            # For training data, add tags\n",
        "            yield gensim.models.doc2vec.TaggedDocument(tokens, [index])\n",
        "\n",
        "# train_corpus = list(read_corpus(doc))"
      ],
      "metadata": {
        "id": "v-PK-NnnJ5DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  model= Doc2Vec.load(\"doc2v.model\")\n",
        "  train_corpus = list(read_corpus(doc))\n",
        "  model.build_vocab(train_corpus, update=True)\n",
        "  model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "  print(len(model.wv.vocab)) \n",
        "  model.save(\"doc2v.model\")\n",
        "  print(\"Model Saved\")\n",
        "  #61113"
      ],
      "metadata": {
        "id": "WmvtXm4g8dOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turn Passage into Vector"
      ],
      "metadata": {
        "id": "pB5sXC1GNx3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# index\n",
        "model= Doc2Vec.load(\"doc2v.model\")\n",
        "collection_vec = copy.deepcopy(collection)\n",
        "collection_vec['passage'] = collection.apply(lambda row : list(model.infer_vector(gensim.utils.simple_preprocess(row['passage']))), axis=1)\n",
        "file_name = \"feature_data/collection_vector.tsv\"\n",
        "with open(file_name ,'w') as write_tsv:\n",
        "    write_tsv.write(collection_vec.to_csv(sep='\\t',index=False))\n"
      ],
      "metadata": {
        "id": "Pb8BeGvhG0mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Round the vec"
      ],
      "metadata": {
        "id": "-5Iryxy5OOJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def turn_to_round(passage):\n",
        "  round_ = passage[1:-1].split(\",\")\n",
        "  round_list=[]\n",
        "  round_list=[ round(float(num), 2) for num in round_]\n",
        "  return round_list\n",
        "\n",
        "collection_vec_round = copy.deepcopy(collection)\n",
        "collection_vec_round = collection_vec.apply(lambda row: turn_to_round(row['passage']), axis=1)\n",
        "file_name = \"feature_data/collection_vec_round.tsv\"\n",
        "with open(file_name ,'w') as write_tsv:\n",
        "      write_tsv.write(collection_vec_round.to_csv(sep='\\t',index=False))"
      ],
      "metadata": {
        "id": "3fKJ5rSMYJUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "qFxWcTn8Acp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# Pick a random document from the test corpus and infer a vector from the model\n",
        "model= Doc2Vec.load(\"doc2v.model\")\n",
        "\n",
        "doc = \"manhattan project\"\n",
        "inferred_vector = model.infer_vector(gensim.utils.simple_preprocess(doc))\n",
        "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
        "\n",
        "# Compare and print the most/median/least similar documents from the train corpus\n",
        "print('Test Document : «{}»\\n'.format(doc))\n",
        "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
        "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
        "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
      ],
      "metadata": {
        "id": "QFafbJdFObvK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Word2Vector.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}